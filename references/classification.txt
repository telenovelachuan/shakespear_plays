Train classification models to determine "Player" based on other features.

1. Logistic Regression
uses multinomial multi_class regressor in sklearn(softmax) to conduct multi-label classification, it trains rather slow due to the huge amount of attributes.
reaches an accuracy of 0.5556 on testing set.

2. Random Forest
Tuned n_estimators to be 20. Trained much faster than LR, and reaches an accuracy of 0.6132 on testing set.
Further increasing the number of trees does not help increase accuracy.

3. SVM
uses the Support Vector Classifier with a "poly" kernel by sklearn. Training takes too much time(more than several days) and never converges...
I think the huge amount of attributes(especially the sparse matrices by one-hot encoding) slows down SVM's kernel trick to find a linear-separable solution in high dimensions.
Besides, the SVC class implemented by sklearn is not specifically optimized such as LinearSVC does.

4. Neural Network
uses Keras to construct and compile a neural network that contains 2 dense hidden layers(100 neurons each) and 1 output layer(more layers barely helps). Stochastic gradient descent is used for backpropagation.
uses "RELU" for activation inside hidden layers, and "softmax" activation for the final output layer since it's a multi-label classification.
60 epochs' training achieves an accuracy of ~0.8 on testing set. Training for more epochs could further improve accuracy, but increases overfitting as well. The NN_training_history.png in reports shows the changing process of accuracy on training data and validation data. It seems the model begins to overfit after 50~60 epochs, maybe some dropout layers or batch normalizations could be added further.

5. XGBoost
Tried this super powerful library based on gradient boosting forest. Unfortunately the model took a time longer than I could wait to converge :(
